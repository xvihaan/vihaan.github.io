---
layout: single
title:  "9.케창딥 - RNN, LSTM"
categories: [AI, ML, DL]
tags: [keras, deeplearning, KDL]
toc: true
author_profile: false
---

# RNN 과 LSTM 차이
---
RNN에서 LSTM으로 발전하면서 **이동 상태**(Cell State)와 **은닉 상태**(Hidden State)의 역할이 RNN과 비교해 크게 개선되었습니다. 이 두 상태는 LSTM에서 핵심적인 기능을 담당하며, **RNN의 한계를 극복**하는 데 중요한 역할을 합니다. RNN과 LSTM의 차이점은 주로 **정보를 어떻게 전달하고 기억하는지**에 관한 것입니다.

## **1. RNN에서의 은닉 상태**

### **RNN의 한계:**
---
RNN에서는 **은닉 상태(Hidden State)** 하나만 존재하며, 각 시간 단계(time step)에서 이전 은닉 상태를 다음 단계로 전달하면서 **순차적인 데이터**를 처리합니다.

• RNN의 은닉 상태는  과거 정보를 현재 상태로 전달하는 역할로 **과거의 정보를 유지**하는 역할을 하지만, 이 정보는 시간이 지남에 따라 손실되기 쉽습니다. 특히, **기울기 소실 문제(Gradient Vanishing Problem)**로 인해 **장기적인 의존성**(long-term dependency)을 학습하는 데 어려움이 있습니다.

• RNN은 시간이 지남에 따라 중요한 정보를 잃어버리며, **긴 시퀀스**에서는 과거의 정보를 효과적으로 유지할 수 없습니다.


## **2. LSTM에서의 이동 상태와 은닉 상태**

LSTM은 RNN의 은닉 상태만으로는 해결할 수 없는 **장기 의존성 문제**를 해결하기 위해, **이동 상태(Cell State)**와 **은닉 상태(Hidden State)**를 별도로 관리하는 구조를 도입했습니다.


**(1) 이동 상태(Cell State):**

• LSTM에서 **이동 상태**(Cell State)는 **장기적인 정보를 유지**하고 **이동**시키는 역할을 합니다. 이는 **장기 기억**과 유사한 기능을 하며, 시간이 지나도 중요한 정보를 잃지 않고 전달할 수 있습니다.

• 이동 상태는 각 시간 단계에서 거의 변하지 않고, LSTM의 게이트들(Forget Gate, Input Gate, Output Gate)에 의해 정보가 선택적으로 유지되거나 잊히게 됩니다.

• 이 구조 덕분에 **장기 의존성**을 처리할 수 있으며, 과거의 중요한 정보가 미래의 예측에 활용될 수 있도록 보존됩니다.

**(2) 은닉 상태(Hidden State):**

• **은닉 상태**는 LSTM에서 **현재 시간 단계의 단기적인 정보**를 나타내며, 출력에 직접적으로 영향을 미칩니다. 은닉 상태는 이동 상태에서 파생된 정보로, 셀 상태와 함께 현재 입력을 바탕으로 새롭게 갱신됩니다.

• 은닉 상태는 RNN에서처럼 각 시간 단계에서 변화하며, LSTM의 출력에 사용됩니다.

  
### **LSTM에서의 역할:**

• **이동 상태(Cell State)**: 장기적인 정보가 전달되는 경로로, 정보를 선택적으로 잊거나 유지할 수 있습니다. 이를 통해 LSTM은 장기적인 패턴을 학습할 수 있으며, 이는 RNN에서는 불가능했던 기능입니다.

• **은닉 상태(Hidden State)**: 현재 시간 단계에서 중요한 정보를 나타내며, 이동 상태로부터 전달받은 정보를 바탕으로 출력에 영향을 미칩니다. 이 은닉 상태는 계속해서 시간 단계가 지남에 따라 업데이트되며, 네트워크의 출력을 결정하는 중요한 요소입니다.


### 결론
• **RNN**에서는 **은닉 상태** 하나만 사용하여 시계열 데이터를 처리하지만, 이 방식은 장기적인 정보를 처리하는 데 한계가 있습니다. 특히, **기울기 소실 문제**로 인해 긴 시퀀스를 학습하는 데 어려움을 겪습니다.

• **LSTM**에서는 **이동 상태(Cell State)**와 **은닉 상태(Hidden State)**를 분리하여 각각 다른 역할을 부여함으로써, **장기 의존성 문제**를 해결했습니다. 이동 상태는 **장기 기억**을 담당하고, 은닉 상태는 **현재 출력과 관련된 정보**를 관리합니다.

따라서, LSTM은 **장기적인 정보**와 **단기적인 정보**를 분리하여 관리함으로써 RNN보다 더 복잡하고 긴 시퀀스를 학습할 수 있습니다.