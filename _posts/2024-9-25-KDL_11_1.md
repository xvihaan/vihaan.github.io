---
layout: single
title:  "10.케창딥 - 트랜스포머"
categories: [AI, ML, DL]
tags: [keras, deeplearning, KDL]
toc: true
author_profile: false
---


# 트랜스포머, 시퀀스 투 시퀀스

##  트랜스포머 아키텍쳐
---

### 어텐션
---

![[Pasted image 20240929192128.png]]

**Attention이란 문맥에 따라 집중할 단어를 결정하는 방식을 의미**

영어 'cafe'와 한국어 '카페'가 강한 Attention 상관관계, 다른 단어들과는 Attention이 약함

#### 어텐션의 구조: 인코더 디코더

![[Pasted image 20240929192501.png]]

- **Encoder**는 입력으로 input data를 받아 압축 데이터(context vector)로 변환 및 출력해주는 역할
- **Decoder**는 반대로 압축 데이터(context vector)를 입력 받아 output data를 출력

왜 이런 구조로 하는지? ⇒  **정보를 압축하므로써 연산량을 최소화하기 위해**

### RNN에 어텐션 개념 도입
---

![[Pasted image 20240929192520.png]]

**유의할 점은 Encoder의 경우 모든 RNN 셀의 hidden states들을 사용하는 반면**

**Decoder의 경우 현재 RNN셀의 hidden state만을 사용**

- **Decoder hidden state**: Target seqence의 문맥
- **Encoder hidden states**: Source seqence의 문맥(모든 문맥을 활용하겠다.)

## 셀프 어텐션

### 목적: 
자기 주의 메커니즘은 입력 시퀀스의 각 요소(보통은 단어의 임베딩)가 다른 요소와 `어떻게 상호 작용하는지를 모델링` 한다. 이는 `Query`, `Key`, `Value` 라는 세가지 다른 표현으로 분해된다. 
- Query는 현재 단어가 어떤 정보에 주의를 기울여야 하는지를 나타내고 
- Key는 각 단어가 어떤 정보를 가지고 있는지를 나타내며, 
- Value는 실제로 그 정보를 어떻게 활용할지를 나타냄.

### 방법: 
세가지 주요 단계로 이루어진다. 각 단계에서는 입력 문장 내의 단어들 간의 상대적인 중요도를 계산하여 가중치를 부여한다.

#### 어텐션 스코어 계산
첫번째 단계에서는 쿼리(Query), 키(key), 벨류(value)라는 세 가지 vector를 생성한다. 이때, query는 현재 단어에 대한 정보를 담고 있고, key와 value는 입력 문장의 다른 단어들에 대한 정보를 담고 있다. 
- query와 key vector간의 유사도를 측정하여 attention score를 계산

우선 Query와 key의 내적(dot product)를 계산하여 attention score를 얻는다. 

이 score는 query와 key 사이의 유사도를 나타내며 높을수록 더 많은 주의를 기울여야 한다.

![[Pasted image 20240929192732.png]]
![[Pasted image 20240929200612.png]]


#### 소프트 맥스(softmax) 함수를 통한 가중치 계산

두 번째 단계에서는 attention score를 확률 분포로 변환하기 위해 소프트맥스 함수를 적용한다. 

이렇게 하면 각 단어의 중요도에 대한 가중치를 얻을 수 있다. 

이 가중치는 Query와 Key사이의 상대적인 중요도를 나타낸다. 

즉 가중치가 높을수록 해당 단어는 문장의 의미를 이해하는 데에 더 큰 영향을 미친다.


#### 가중치를 적용한 밸류 계산
세 번째 단계에서는 소프트맥스 함수를 통해 얻은 가중치를 Value vector에 곱샘 적용하여 최종적인 단어 표현을 계산한다. 

즉 각 단어의 value vector를 가중치와 곱한 뒤 합산하여 **Self Attention**의 결과를 얻게 된다. 이를 통해 문맥을 고려한 표현

![[Pasted image 20240929235556.png]]

장점: 
1. **병렬 계산 가능(elf Attention**은 모든 요소를 동시에 처리할 수 있다. 이로 인해 병렬 계산이 가능하며, 연산 속도가 빠르다.**)**

2. **문장 길이에 영향 받지 않음**
	모든 단어 간의 관계를 한 번에 계산하기 때문에 문장의 길이에 상관 없이 일정한 계산 시간을 유지

3. **문맥 파악 능력 강화**
	문장 내의 각 단어가 다른 단어와 어떻게 상호 작용하는지를 정밀하게 파악한다. 이로 인해 문맥을 더 잘 이해할 수 있으며, 이는 특히 고정된 윈도우 크기의 CNN이나 단기 메모리의 RNN보다 뛰어나다.
![[Pasted image 20240929235734.png]]

#### 어텐션 메커니즘
![[Pasted image 20240929235750.png]]



## 멀티 헤드 어텐션
---

**멀티헤드 어텐션**은 **셀프 어텐션**을 확장한 개념으로, 여러 개의 **어텐션 헤드**를 사용해 서로 다른 차원에서 다양한 패턴을 학습

이 기법은 트랜스포머 모델의 성능을 높이는 데 중요한 역할을 담당

### 주요 특징:
---
- **여러 개의 어텐션**을 동시에 수행합니다. 각 어텐션 헤드는 **쿼리, 키, 값** 벡터를 다르게 학습하여, 서로 다른 관계를 포착
- 이로 인해, 모델은 **다양한 문맥 정보**를 더 풍부하게 학습할 수 있음

### 동작 방식:
---
1. 입력 시퀀스에 대해 **여러 개의 어텐션 헤드**를 병렬로 수행하여, 서로 다른 쿼리, 키, 값 벡터를 사용
2. 각 헤드는 독립적으로 어텐션을 계산하며, 최종적으로 **헤드들의 결과를 결합**하여 최종 출력을 만듬

⇒ 멀티헤드 어텐션은 하나의 어텐션 메커니즘만 사용했을 때 놓칠 수 있는 **다양한 관계**를 학습할 수 있는 장점

`[4x4]` 크기의 문장 임베딩 벡터와 `[4x8]` 의 `Query, Key, Value` 가 있을 때, 일반적인 한 번에 계산하는 Attention 메커니즘은 `[4x4]*[4x8]=[4x8]`의 Attention Value가 한 번에 도출

- 일반적인 어텐션 메커니즘
![[Pasted image 20240929235919.png]]

- 멀티 헤드 어텐션 메커니즘
![[Pasted image 20240929235904.png]]


## **트랜스포머 인코더(Transformer Encoder)**
---

**트랜스포머 인코더**는 트랜스포머 모델에서 **입력 시퀀스를 처리하여 고차원 벡터로 변환**하는 역할

인코더는 트랜스포머의 **N개의 블록**으로 구성되며, 각각의 블록은 **멀티헤드 어텐션**과 **피드포워드 신경망 구성**

### 주요 구성 요소:
---
- **멀티헤드 어텐션**: 입력 시퀀스의 각 단어가 **다른 단어들과의 관계**를 학습
- **피드포워드 네트워크(Feedforward Network)**: 어텐션을 통해 학습된 정보를 **비선형 변환**을 통해 추가적으로 처리
- **잔차 연결(Skip Connection)**: 각 레이어는 이전 레이어의 출력을 **직접 연결**하여 정보 손실을 방지하고 학습을 안정화

인코더는 입력 시퀀스의 모든 단어가 서로의 관계를 학습하여 **고차원 벡터 표현**을 생성하며, 이는 디코더로 전달됩니다.

![[Pasted image 20240930000014.png]]




## 텍스트 분류를 넘어: 시퀀스-투-시퀀스 학습


### 시퀀스 투 시퀀스(Seq2Seq) 모델
---

- **시퀀스 투 시퀀스 모델은 입력 시퀀스를 다른 시퀀스로 변환**하는 모델
    - 입력으로 문장이나 문단을 받아 이를 다른 시퀀스로 바꾼다는 의미
    - 예를 들어, 기계 번역, 텍스트 요약, 대화 생성과 같은 작업에서 사용

### 주요 개념:
---
- **인코더-디코더 구조**: 입력 시퀀스를 **인코더**가 처리하여 **고차원 벡터**로 변환한 후, 이를 **디코더**가 받아 **출력 시퀀스**를 생성

### 동작 방식:
---
1. **인코더**는 입력 시퀀스를 처리하여 하나의 고차원 벡터로 변환
2. **디코더**는 이 벡터를 받아, 순차적으로 단어를 생성하여 출력 시퀀스를 만듦

![[Pasted image 20240930000119.png]]

### RNN을 사용한 시퀀스 투 시퀀스 모델
---
- **RNN 기반 Seq2Seq 모델**은 RNN 구조를 사용하여 **입력 시퀀스를 출력 시퀀스로 변환**하는 모델
- RNN은 순차적 데이터를 처리하는 데 강력한 성능을 보이며, **시간적 의존성**을 학습

### 주요 구성 요소:
---
- **RNN 인코더**: 입력 시퀀스를 처리하여 마지막 은닉 상태를 추출
- **RNN 디코더**: 인코더의 마지막 은닉 상태를 받아, 순차적으로 출력 시퀀스를 생성

### 동작 방식:
---
- 입력 시퀀스를 한 번에 하나씩 처리하면서 은닉 상태를 업데이트하며, 인코더의 마지막 은닉 상태가 디코더에 전달되어 출력 시퀀스를 생성하는 데 사용

![[Pasted image 20240930000147.png]]

### **트랜스포머 Seq2Seq** 모델
---
- 전통적인 RNN 기반 모델과 달리 **트랜스포머 구조**를 사용하여 **입력 시퀀스를 출력 시퀀스로 변환**
- RNN과 달리 트랜스포머는 **병렬 처리**가 가능하여 학습 속도가 빠르고, 더 긴 시퀀스를 효과적으로 처리

### 주요 구성 요소:
---
- **인코더**: 입력 시퀀스를 **멀티헤드 `어텐션**과 **피드포워드 네트워크**`를 통해 처리하여 고차원 벡터로 변환
    - **`어텐션**과 **피드포워드 네트워크` 란?**
        - **트랜스포머 블록** 내에서 순차적으로 사용되며, 이 두 가지가 결합되어 트랜스포머가 문맥을 더 깊이 이해하고, 시퀀스를 더 효과적으로 처리할 수 있게 함
- **디코더**: 인코더의 출력을 바탕으로 **출력 시퀀스**를 생성하며, **멀티헤드 어텐션**을 통해 인코더에서 전달된 정보를 활용

