---
layout: single
title:  "2.케창딥 - Tensor"
categories: [AI, ML, DL]
tags: [keras, deeplearning, KDL]
toc: true
author_profile: false
---


# 신경망의 수학적 구성 요소

## **신경망을 위한 데이터 표현**
- 텐서, 랭크, 슬라이싱, 배치, 사례(벡터, 시계열, 이미지, 동영상)
### **텐서**란?
- 수치형 데이터를 다루기 때문에 숫자를 위한 컨테이너
- 임의의 차원의 개수를 가지는 행렬이 일반화된 모습
- `ndim`을 사용하면 넘파이 배열의 축을 확인할 수 있는데 텐서의 축의 개수를 랭크라 부름
- 축과 벡터의 개념을 달리 생각해야함

### 스칼라(scalar)
**스칼라**는 단일 숫자로, **0차원 텐서**로 표현됩니다. **차원이 없는** 단일 값을 나타냄

예시로는 온도, 나이, 길이 등 하나의 값만 있는 경우
- 하나의 숫자만 담고 있는 텐서
- 축의 개수 : 0

### 벡터
**벡터**는 하나의 차원을 가지는 **1차원 텐서**입니다. 이는 여러 개의 스칼라 값을 배열로 표현한 것

벡터는 크기와 방향을 가지며, 배열 내의 값들은 동일한 특성을 나타냄

벡터는 수학적으로 한 줄로 나열된 값들의 집합. 예를 들어, [1, 2, 3]은 1차원 벡터
- 배열, 리스트 등 1차원 데이터
- 하나의 축을 가짐

### 행렬
**행렬**은 2차원 텐서로, **행(row)** 과 **열(column)** 로 구성된 숫자들의 배열

행렬은 벡터가 2차원으로 확장된 형태로 생각할 수 있습니다. 예를 들어, 2x2 행렬은 다음과 같다.
• 예시: `[[1, 2], [3, 4]]` (2x2 행렬)
• 행렬은 **2차원 데이터**를 표현하는 데 사용됩니다. 이미지나 테이블 형식의 데이터가 여기에 속합니다.
- matrix , 두개의 축
- 행과열

### 그 이상의 텐서 (고차원)
**고차원 텐서**는 3차원 이상의 데이터를 표현하며, 여러 차원을 갖는 배열

예를 들어, **이미지 데이터**는 주로 3차원 또는 4차원 텐서로 표현
• 3차원 텐서: `[depth, height, width]` (예: 컬러 이미지)
• 4차원 텐서: `[batch, depth, height, width]` (예: 여러 이미지가 포함된 데이터셋)
- 랭크 3 텐서들을 하나의 배열로 합치면 랭크 4 텐서가 됨
- 동영상 (랭크 5 텐서)

### 텐서의 핵심속성
1. **차원(rank)**
	1. 스칼라는 **0차원 텐서**
	2. 벡터는 **1차원 텐서**
	3. 행렬은 **2차원 텐서**
	4. 그 이상의 차원도 가능(3차원, 4차원 텐서 등)
2. **형상(shape)**
	1. 텐서의 **형상**은 각 차원의 크기. 즉, 텐서의 구조를 정의하는 속성
	2. 예를 들어, 2x3 행렬의 형상은 (2, 3)
	3. 텐서플로우에서 텐서의 형상 확인: `print(matrix.shape)`
3. **데이터 타입(data type)**
4. **크기(size)**

#### 요약
1. **스칼라 (Scalar)**: 단일 값으로, 0차원 텐서입니다.
2. **벡터 (Vector)**: 여러 개의 숫자로 이루어진 1차원 텐서입니다.
3. **행렬 (Matrix)**: 행과 열로 이루어진 2차원 텐서입니다.
4. **고차원 텐서**: 3차원 이상의 다차원 데이터를 표현하는 텐서입니다.

### 넘파이로 텐서 조작하기
- 코드

```python
slice = train_image[10:100] # 행만 슬라스이
d_3_slice = train_image[10:100, :,: ] # 위의 코드를 풀어 쓴 것

```

### 배치 데이터
데이터를 나눌 때 기준 축을 말함
![[Pasted image 20240917155050.png]]


## **신경망의 톱니바퀴**
### 원소별 연산
relu 함수와 덧셈은 `원소별 연산(element-wise operation)` 이다.

이 연산은 텐서에 있는 각 원소에 독립적으로 적용

넘파이 배열을 다룰 떄는 최적화된 넘파이 내장 함수로 이런 연산들을 처리 가능

넘파이는 BLAS(Basic Linear Alegebra Subprogram) 구현에 복잡한 일들을 위함

BLAS 는 고도로 병렬화되고 효율적인 저수준의 텐서 조작 루틴.

전형적으로 포트란(Fortran)이나 C 언어로 구현

텐서플로 코드를 GPU에서 실행할 때 고도로 병렬화된 GPU 칩 구조를 최대로 활용할 수 있는 완전히 벡터화된 CUDA 구현을 통해 원소별(element-wise) 연산이 실행

### 브로드캐스팅
모호하지 않고 실행 가능하다면 작은 텐서가 큰 텐서의 크기에 맞추어 `브로드캐스팅(broadcasting)` 된다.
브로드캐스팅은 두 단계로 이뤄짐.
1. 큰 텐서의 ndim에 맞도록 작은 텐서에 (브로드캐스팅 축) 축이 추가 된다.
2. 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복

구현 입장에서는 새로운 텐서가 만들어지면 매우 비효율적이므로 어떤 랭크-2 텐서도 만들어지지 않음.

반복된 연산은 완전히 가상적이고 이 과정은 메모리 수준이 아니라 알고리즘 수준에서 일어남.

하지만 새로운 축을 따라 벡터가 32번 반복된다고 생각하는 것으로 이해해야 쉽다.

#### 크기가 다른 투 텐서에 브로드캐스팅
원소별 `maximum` 연산을 적용하는 예입니다.

```python
x = np.random.random((64, 3, 32, 10))
y = np.random.random((32, 10))
z = np.maximum(x, y)  # 출력 z크기는 x와 동일한 (64, 3, 32, 10)
```

### 텐서 곱셈
텐서 곱셈(tensor product) 또는 점곱(dot product) 은 가장 널리 사용되고 유용한 텐서 연산
- 넘파이에서 텐서 곱셈은 `np.dot` 함수를 사용해 수행

```
x = np.random.random((32,))
y = np.random.random((32,))
z = np.dot(x, y)
```

점곱 연산은 수학에서 어떤 일을 할까요? 2개의 벡터 x와 y의 점곱은 두 벡터의 두 벡터의 점곱은 스칼라가 되므로 원소 개수가 같은 벡터끼리 점곱이 가능한다.

행렬 x와 벡터 y 사이에서도 점곱이 가능하다. y와 x의 행 사이에서 점곱이 일어나므로 벡터가 반환된다.

