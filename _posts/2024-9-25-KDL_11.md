---
layout: single
title:  "11.케창딥 - 자연어 처리"
categories: [AI, ML, DL]
tags: [keras, deeplearning, KDL]
toc: true
author_profile: false
---


# 자연어 처리 소개
NLP 시스템은 트랜스포머를 기반

## 텍스트 데이터 준비
---

![[Pasted image 20240929191528.png]]

### 단어 수준 토큰화
---
토큰이 공백으로 구분된 부분 문자열

- `staring` ⇒ `start+ing` , `called` ⇒ `call+ed`

### N-그램 토큰화
---
토큰이 N개의 연속된 단어 그룹

- `the cat`, `he was`

### 문자 수준 토큰화
---
각 문자가 하나의 토큰

- 시퀀스 모델: 단어의 순서를 고려
    - `단어 수준 토큰화`를 사용
- Bow 모델: 순서를 고려하지 않고(무시) 집합으로 다룸
    - `N-그램 토큰화`를 사용 - 국부적인 단어 순서에 대한 소량의 정보를 주입하는 방법

### 어휘 사전 인덱싱
---
문제: 어휘 사전 인덱스에서 새로운 토큰을 찾을 때 이 토큰이 항상 존재하지 않을 수도 있다.

해결: `예외 어휘(out of indexing)` 인덱싱 사용(OOV 인덱스)

- 어휘 사전에 없는 모든 토큰에 대응.
- 일반적으로 1 (`token_index = vocabulary.get(token, 1)` )과 같다.
    - 0은 이미 사용되는 토큰이기에 1을 사용. 마스킹 토큰(인덱스0)
    - OOV 토큰: `인식할 수 없는 단어` / 마스킹 토큰: `단어가 아니라 무시할 수 있는 토큰` 뜻

### TextVectorization 층
---
텍스트 표준화를 위해 소문자로 바꾸고 구두점을 제거하며 토큰화를 위해 공백으로 나눔

- 사용자 정의 함수: `tf.string` 텐서를 처리

## 단어 그룹을 표현하는 두 가지 방법: 집합과 시퀀스
---

### 단어를 집합으로 처리하기: BoW 방식
---
**BoW 모델** : 단어의 순서는 무시하고 텍스트를 단어의 집합으로 처리하는 모델

개별 단어(unigram)를 사용하거나 연속된 토큰 그룹(N-gram)을 사용해 국부적인 순서 정보를 유지할 수 있다.

### 이진 인코딩을 사용한 유니그램
---
인코딩은 전체 텍스트를 하나의 벡터로 표현

- 멀티-핫(muliti-hot) 이진 인코딩(binary encoding):
    - 하나의 텍스트를 **어휘 사전에 있는 단어 개수만큼** 차원을 가진 벡터로 인코딩
    - 텍스트 단어 차원 = 1, 나머지 = 0 `하나의 단어씩 처리(유니그램)`

### 이진 인코딩을 사용한 바이그램
---
문제: `Unites States` 는 `states` 와 `united` 단어의 개별적 의미와 많이 다른 개념 제공

해결: 단일 단어가 아닌 `N-gram` 을 사용해 국부적인 순서 정보를 `BoW` 표현에 추가(바이그램)

- 바이그램 예시
`{"the", "the cat", "cat", "cat on", "on", "on the", ...}`

### TF-IDF 인코딩을 사용한 바이그램
---
개별 단어나 N-gram의 등장 횟수를 카운트한 정보를 추가

- TF-IDF 정규화 이해
    - `the`나 `a`와 같이 거의 모든 문장에 사용되는 단어는 유용하지 않다.
    - 반면, 전체 문장들 중 일부에서만 나타나는 단어는 매우 독특하므로 중요하다.
    - 현재 문서에 단어가 등장하는 횟수인 **단어빈도**로 해당 단어에 가중치를 부여하고, 데이터셋 전체에 단어가 등장하는 횟수인 **문서빈도**로 나눈다.

![[Pasted image 20240929191818.png]]

### 단어를 시퀀스로 처리하기: 시퀀스 모델 방식
---
**시퀀스 모델** : 순서 기반의 특성을 수동으로(바이그램과 같은) 만드는 대신 원시 단어 시퀀스를 모델에 전달하여 스스로 이러한 특성을 학습하도록 하는 방법을 사용한 모델

### 단어 임베딩 이해하기
---
서로 다른 의미를 갖는 단어는 서로 멀리 떨어져 있고 관련이 있는 단어는 가까이 놓여 있도록 한다.

- **원-핫 인코딩**:
    - 대부분이 0이라 **희소**하고, **고차원**인 이진 벡터를 만든다.
        
        ![[Pasted image 20240929191848.png]]
        
- **단어 임베딩**:
    
    - **저차원**의 부동 소수점 벡터로 만든다.
        
    - 더 많은 정보를 더 적은 차원으로 압축한다.
        
    - 구조를 데이터로부터 학습한다.
        
        - 비슷한 단어는 가까운 위치에 임베딩된다.
        - `movie` 와 `film` 의 경우 거의 동일한 의미로 해석되므로 두 단어의 벡터는 매우 가까워야 한다.
    - 임베딩 공간에서 의미 있는 기하학적 변환의 일반적인 예
        
        - `성별` 벡터 : `king` 벡터 + `female` 벡터 = `queen` 벡터
        - `plural` 벡터 : `king` 벡터 + `plural` 벡터 = `kings` 벡터
        
        ![[Pasted image 20240929191903.png]]
        
    - 단어 임베딩을 만드는 방법
        
        - 현재 작업과 함께 단어 임베딩을 학습한다.
        - 사전 훈련된 단어 임베딩을 모델에 불러온다.

### Embedding 층으로 단어 임베딩 학습하기
---
특정 단어 → 정수 인덱싱 → 밀집 벡터

작업에 따라 새로운 임베딩을 학습하는 것이 타당하다.
- 예로 **영화 리뷰의 감성 분석**에서의 단어 임베딩 공간과 **법률 문서 분류**에서의 임베딩 공간과 다르기 때문이다.

`layers.Embeding` :
- 입력 : (batch_size, sequence_length)
- 출력 : (batch_size, sequence_length, embedding_dimensionality)
- 가중치 :
    - 다른 층들과 마찬가지로 랜덤하게 초기화한다.
    - 역전파를 통해 점차 조정하고 후속 모델을 위한 임베딩 공간을 생성한다.

### 패딩과 마스킹 이해하기
---
TextVectorization 층 `output_sequence_length = 600` :
- 600개의 토큰보다 긴 문장은 600개의 토큰 길이로 잘라버린다.
- 600개의 토큰보다 짧은 문장은 0으로 채워버린다.

Embedding 층은 입력 데이터에 상응하는 masking을 생성
- masking은 0 또는 1로 이루어져있고 0 또는 False이면 이 타임스템을 건너뛰고 그렇지 않으면 처리한다.

### 사전 훈련된 단어 임베딩 사용하기
---
Word2vec

![[Pasted image 20240929191951.png]]

GloVe
`trainable=False` !