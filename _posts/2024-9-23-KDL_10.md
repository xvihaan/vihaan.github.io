---
layout: single
title:  "8.케창딥 - 순환 신경망(RNN)"
categories: [AI, ML, DL]
tags: [keras, deeplearning, KDL]
toc: true
author_profile: false
---


# 시계열 데이터, RNN, LSTM
## 다양한 종류의 시계열 작업

### 시계열로 할 수 있는 작업
---
#### 분류
- 예: 웹 사이트의 방문자 활동에 대한 시계열이 주어지면 이 방문자가 봇인지 사람인지 분류합니다.
#### 이벤트 감지
- 연속된 데이터 스트림에서 예상되는 특정 이벤트 발생을 식별합니다.
- 사용사례 : 핫워드 감지(모델이 오디오 스트림을 모니터링하다가 특정 단어를 감지)
#### 이상치 탐지
-  연속된 데이터 스트림에서 발생하는 비정상적인 현상을 감지합니다.

## 온도 예측 문제
### 시계열 데이터에서 항상 주기성 찾기!
---
- 여러 시간 범위에 걸친 주기성은 시계열 데이터에서 중요하고 매우 일반적인 성질이다.
- 일별, 주별, 연간 등과 같은 특정 시간 단위에 따라 주기성을 찾을 수 있다.

### 시계열 데이터 분할
---
- 시계열 데이터를 다룰 때 검증 데이터와 테스트 데이터가 훈련 데이터보다 최신이어야 한다.
- 시계열 데이터 교차 검증![[Pasted image 20240928212641.png]]

Q. 그렇다면 시계열 데이터의 교차 검증이 끝난 후 테스트 데이터를 예측할 때, 전에 배웠던 것과 동일하게 전체 시계열 데이터를 사용해서 테스트 데이터를 예측해야 할까?
- 권장한다!
- 시계열 데이터의 경우 최근 데이터일 수록 더 중요한 데이터이기 때문이다.
- 교차 검증을 통해 하이퍼 파라미터를 최적화시키고 전체 데이터를 사용해 테스트 데이터를 예측하는 것을 권장한다.
- 도메인과 데이터 특성, 패턴에 따라서 전체 데이터를 사용할지 일부 구간만 정해서 사용할지, 정해진 것은 없고 결정에 따라 다르다고 본다.

![[Pasted image 20240928212714.png]]
`sampling_rate` = 9개( 예) 파란선)

데이터 포인트에 해당되지 않는 데이터들은 누락되는 것이 아니라 데이터의 대표성에 대해 누락됐다는 의미. 타겟 데이터에는 사용되니 데이터의 의미는 그대로 중요하게 사용

### 랜덤 샘플링
---
- 훈련 데이터를 순서대로 학습하는 것보다 랜덤하게 샘플링하는 것이 지역 최솟값에서 벗어나기 쉽고 과대적합을 줄일 수 있다.
- 시계열 데이터의 경우 임의의 위치에서 시퀀스를 추출한다.

### 1D 컨브넷의 문제점
---
- 평행 이동 불변성 가정을 많이 따르지 않는다.
- 데이터의 순서가 많이 중요하다.


## 순환 신경망 이해하기

### 순환 신경망(Recurrent Neural Network, RNN)
---
과거 정보를 사용하여 구축되며 새롭게 얻은 정보를 계속 업데이트한다.

시퀀스의 원소를 순회하며 지금까지 처리한 정보를 상태(state)에 저장한다.

rnn의 state는 2개의 다른 시퀀스를 처리하는 사이에 재설정된다.

![[Pasted image 20240928212815.png]]

단점: 순환 신경망의 기억력 한계 - 그레디언트 소실 문제

**장기 문맥 의존성(멀리 떨어진 요소가 밀접한 상호작용하는 현상)** 을 제대로 처리하지 못하는 한계

예) (not ~~~~~~~ good) 이런 식으로 되어있다면 분명 좋지 않다는 의미인데, not을 제대로 기억하지 못해서 오히려 좋다고 판단해버리는 경우가 발생(계속 들어오는 input의 영향으로 기억력 감퇴)

### LSTM(Long Short-Term Memory)
---
RNN의 그레디언트 소실 문제를 보완했다.

과거의 입력 데이터를 지속적으로 입력해주는 $C_t$(cell state)가 추가되어 RNN에 비해 긴 시퀀스의 입력을 처리하는데 탁월한 성능을 보인다.
- 삭제 게이트(forget gate)
- 입력 게이트(input gate)
- 출력 게이트(output gate)
- 게이트를 여닫는 정도는 가중치로 표현, 가중치는 학습으로 알아냄
![[Pasted image 20240928212853.png]]

#### LSTM 구조
1. forget gate layer
	![[Pasted image 20240928212940.png]]
- 현재 시점 $C_t$ 과 이전 시점 $h_{t-1}$의 은닉 상태가 시그모이드 함수를 지난다.
- 시그모이드 함수를 거친 0과 1 사이의 값이 삭제 과정을 거친 정보의 양이다.

2. input gate layer
![[Pasted image 20240928213045.png]]

3. cell state 업데이트
![[Pasted image 20240928213119.png]]

4. output gate layer
![[Pasted image 20240928213142.png]]
- 유닛 개수를 늘린다 → 가설공간에 제약조건을 건다
- 제약조건을 건다 → 과대적합을 방지


## 순환 신경망의 고급 사용법

### 순환 드롭아웃(recurrent dropout)
---
- 드롭아웃의 한 종류로 순환 층에서 과대적합을 방지하기 위해 사용한다.
- 타임스텝마다 랜덤하게 드롭아웃 마스크를 바꾸는 것이 아니라 동일한 드롭아웃 마스크를 모든 타임스텝에 적용해야 한다.
- 네트워크가 학습 오차를 타임스텝에 걸쳐 적절하게 전파할 수 있다.

### 스태킹 순환 층(stacking recurrent layer)
---
- 성능상 병목이 있어 네트워크의 용량과 표현력을 늘려야함(유닛개수, 층 추가)
→ 모델의 표현 능력을 증가시킨다. (계산 비용이 많이 든다.)

![[Pasted image 20240928213225.png]]
- `return_sequences=True`


### 양방향 순환 층(bidirectional recurrent layer)
---
순환 네트워크에 같은 정보를 다른 방향으로 주입하여 정확도를 높이고 기억을 좀 더 오래 유지시킨다.

자연어 처리의 경우 문장을 이해하는 데 단어의 중요성은 단어가 문장의 어느 위치에 놓여 있는지에 따라 결정되지 않는다. 즉, 언어를 이해하는 데 단어의 순서가 중요하지만 결정적이지는 않다.

그리하여 입력 시퀀스를 양쪽 방향으로 바라보기 때문에 잠재적으로 풍부한 표현을 얻고 시간 순서대로 처리할 때 놓칠 수 있는 패턴을 감지

![[Pasted image 20240928213314.png]]

단점: 성능은 시간 순서대로 처리하는 네트워크 절반에서 온다. 특히 시간 반대 순서로 처리하는 절반은 `온도 예측 작업` 에서 성능이 매우 좋지 않다 또한 층이 두개이기에 용량이 2배가 되어 훨씬 일찍 과적합이 시작

#### `layers.Bidirectional` 
- `layers.Bidirectional(layers.LSTM())(inputs)` 을 사용하면 새로운 두 개의 `LSTM` 을 만든다.
- 하나의 `LSTM`은 `inputs` 에 대해 시간 순서대로 처리하고, 다른 하나의 `LSTM` 은 반대 순서로 처리한다.

#### 이동 상태를 개선 `C_t` 정확이 무엇을 의미?
---
- 이동상태
    - LSTM에서 데이터를 시간에 따라 전달하거나 중요한 정보를 유지하거나 잊어버리는 역할을 하는 일종의 `장기기억` 이라고 함
    - 이동상태의 개선은 각 타임스텝에서 셀 상태를 관리하여 장기적인 의존성을 학습하고 과거 정보를 소실하지 않은 것

#### RNN의 경우 활성화 함수를 tanh를 사용하는 이유
---
RNN에서 주로 tanh를 사용하는 이유는, RNN의 Vanishing gradient 문제를 예방하기 위해서 gradient가 최대한 오래 유지될 수 있도록 해주는 역할로 tanh가 적합하기 때문
- sigmoid의 경우 곱셈이 거듭되면서 Vanishing gradient 문제가 생길 가능성이 상대적으로 크기 때문에 tanh를 사용하는 것이 gradient를 잃지 않는데 유리

#### RNN과 LSTM 이동상태/은닉상태 역할
---
RNN
- **RNN**에서는 **은닉 상태** 하나만 사용하여 시계열 데이터를 처리하지만, 이 방식은 장기적인 정보를 처리하는 데 한계가 있습니다.
- 특히, **기울기 소실 문제**로 인해 긴 시퀀스를 학습하는 데 어려움을 겪습니다.

LSTM
- **LSTM**에서는 **이동 상태(Cell State)**와 **은닉 상태(Hidden State)** 를 분리하여 각각 다른 역할을 부여함으로써, **장기 의존성 문제**를 해결했습니다.
- 이동 상태는 **장기 기억**을 담당하고, 은닉 상태는 **현재 출력과 관련된 정보**를 관리합니다.

요약)
- LSTM은 장기적인 정보와 단기적인 정보를 분리하여 관리함으로써 RNN보다 더 복잡하고 긴 시퀀스를 학습할 수 있다.